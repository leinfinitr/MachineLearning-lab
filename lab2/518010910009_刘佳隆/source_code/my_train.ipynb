{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "该 ipynb 文件构建了一个基于 transformers 的模型训练框架，用于训练 GPT2 模型。\n",
    "\n",
    "该框架包含五个部分：\n",
    "\n",
    "- 模型：定义模型的结构\n",
    "- 评估器：评估模型的性能\n",
    "- 训练器：定义模型的训练过程\n",
    "- 数据处理器：处理数据\n",
    "- 训练：调用训练器进行训练。\n",
    "\n",
    "受限于笔记本配置，本地训练较慢，因此迁移至 Google Colab 进行训练。在第五部分添加了 Google 云端硬盘的挂载，将数据集和模型保存至云端硬盘。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f3f846431ed1ff3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第一步：定义模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecdd97294188ed93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=logging.BASIC_FORMAT,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class GPTSingleHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Different from directly using GPT2LMHeadModel, this wraps up GPT2LMHeadModel as well as GPT2Tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: str, max_seq_length: int = 256, do_lower_case: bool = False,\n",
    "                 special_words_to_add=None):\n",
    "        \"\"\"\n",
    "        定义了一个名为 GPTSingleHead 的 PyTorch 模型类，用于创建 GPT2 模型\n",
    "        :param model_name_or_path: 指定要加载或初始化的 GPT2 模型的名称或路径。\n",
    "        :param max_seq_length: 指定输入序列的最大长度。\n",
    "        :param do_lower_case: 指定是否将输入文本转换为小写。\n",
    "        :param special_words_to_add: 一个可选参数，用于指定要添加到 tokenizer 中的特殊词语。如 <python>, <java>\n",
    "        \"\"\"\n",
    "        super(GPTSingleHead, self).__init__()\n",
    "        self.config_keys = ['max_seq_length', 'do_lower_case']\n",
    "        self.do_lower_case = do_lower_case\n",
    "        if max_seq_length > 1024:\n",
    "            logging.warning(\n",
    "                \"GPT only allows a max_seq_length of 1024. Value will be set to 1024\")\n",
    "            max_seq_length = 1024\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path, do_lower_case=do_lower_case)\n",
    "        if special_words_to_add != None:\n",
    "            self.add_special_words(special_words_to_add)\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        # self.pad_token_id=self.tokenizer.pad_token_id\n",
    "\n",
    "    def tokenize(self, text: str):  # default for cls\n",
    "        \"\"\"\n",
    "        将输入文本转换为 token IDs 的序列。\n",
    "        首先使用 tokenizer.tokenize 将文本标记化为 token 列表，然后使用 tokenizer.convert_tokens_to_ids 将 token 列表转换为对应的 token IDs。\n",
    "        :param text: 输入文本\n",
    "        :return: token IDs 的序列\n",
    "        \"\"\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "\n",
    "    def add_special_words(self, special_words_to_add):\n",
    "        \"\"\"\n",
    "        添加特殊词语到 tokenizer 中，并调整模型的 token embeddings 大小以适应新的词汇量。\n",
    "        :param special_words_to_add: 要添加到 tokenizer 中的特殊词语\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        orig_num_tokens = len(self.tokenizer)\n",
    "        num_added_tokens = self.tokenizer.add_special_tokens(special_words_to_add)\n",
    "        if num_added_tokens > 0:\n",
    "            self.gpt.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "    def forward(self, input: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        定义模型的前向传播逻辑。接收一个名为 input 的字典作为输入，包含键为\"input_ids\"的输入token IDs。\n",
    "        使用 self.gpt 模型将 input[\"input_ids\"]作 为输入，并返回损失和 logit（模型的输出）。\n",
    "\n",
    "        :param input: 一个字典，包含键为\"input_ids\"的输入token IDs。\n",
    "        :return:  损失和 logit（模型的输出）\n",
    "        \"\"\"\n",
    "        loss, logits = self.gpt(input[\"input_ids\"], labels=input[\"input_ids\"])[:2]\n",
    "        return loss, logits\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        \"\"\"\n",
    "        返回模型的配置字典，该字典包含在初始化函数中定义的配置参数。\n",
    "        \"\"\"\n",
    "        return {key: self.__dict__[key] for key in self.config_keys}\n",
    "\n",
    "    def padding_features(self, features_dict_list):\n",
    "        \"\"\"\n",
    "        padding features for a batch\n",
    "        对一个batch的特征进行padding。\n",
    "        遍历features_dict_list中的每个特征字典，将每个特征的token IDs加入到对应的batch列表中。\n",
    "        找到batch中最长的输入序列长度max_input_len_this_batch。\n",
    "        根据max_input_len_this_batch对每个特征的token IDs进行padding，使其长度一致。\n",
    "        :param features_dict_list: i.e., batch\n",
    "        :return: padded batch features\n",
    "        \"\"\"\n",
    "        max_input_len_this_batch = 0\n",
    "\n",
    "        batch_features = {feature_name: [] for feature_name in features_dict_list[0]}\n",
    "        for feature_dict in features_dict_list:\n",
    "            for feature_name, feature_ids in feature_dict.items():\n",
    "                if feature_name == \"input_ids\" and len(feature_ids) > max_input_len_this_batch:\n",
    "                    max_input_len_this_batch = len(feature_ids)\n",
    "                batch_features[feature_name].append(feature_ids)\n",
    "\n",
    "        padded_batch_features = {feature_name: [] for feature_name in features_dict_list[0]}\n",
    "        for feature_name, batch_ids in batch_features.items():\n",
    "\n",
    "            for each_ids in batch_ids:\n",
    "                padded = each_ids + [self.tokenizer.pad_token_id] * (max_input_len_this_batch - len(each_ids))\n",
    "                padded_batch_features[feature_name].append(padded)\n",
    "\n",
    "        for feature_name, ids in padded_batch_features.items():\n",
    "            padded_batch_features[feature_name] = torch.tensor(ids)\n",
    "\n",
    "        return padded_batch_features\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        返回模型的嵌入维度。\n",
    "        \"\"\"\n",
    "        return self.gpt.config.hidden_size\n",
    "\n",
    "    def get_config(self) -> int:\n",
    "        \"\"\"\n",
    "        返回模型的配置。\n",
    "        \"\"\"\n",
    "        return self.gpt.config\n",
    "\n",
    "    def save(self, output_path: str):\n",
    "        \"\"\"\n",
    "        保存模型的权重、tokenizer和配置字典到指定路径。\n",
    "        :param output_path: 模型保存路径\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.gpt.save_pretrained(output_path)\n",
    "        self.tokenizer.save_pretrained(output_path)\n",
    "        with open(os.path.join(output_path, 'gpt_sh_config.json'), 'w') as f:\n",
    "            json.dump(self.get_config_dict(), f, indent=2)\n",
    "\n",
    "    def reload(self, input_path: str):\n",
    "        \"\"\"reload from checkpoint weights\"\"\"\n",
    "        return GPTSingleHead.load(input_path + \"/0_GPTSingleHead\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path: str):\n",
    "        if not os.path.isfile(os.path.join(input_path, 'gpt_sh_config.json')):\n",
    "            raise ValueError(\"In the model path does not find gpt_sh_config.json file, you may have not trained yet\")\n",
    "        with open(os.path.join(input_path, 'gpt_sh_config.json')) as f:\n",
    "            config = json.load(f)\n",
    "        return GPTSingleHead(model_name_or_path=input_path, **config)\n",
    "\n",
    "\n",
    "class EmptyHeads(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.config_keys = []\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: Dict[str, Tensor]):\n",
    "        return input\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {key: self.__dict__[key] for key in self.config_keys}\n",
    "\n",
    "    def save(self, output_path):\n",
    "        with open(os.path.join(output_path, 'empty_heads_config.json'), 'w') as f:\n",
    "            json.dump(self.get_config_dict(), f, indent=2)\n",
    "        torch.save(self.state_dict(), os.path.join(output_path, 'empty_heads.pt'))\n",
    "\n",
    "    def load_saved(self, input_path):\n",
    "        self.load_state_dict(torch.load(os.path.join(input_path, '1_EmptyHeads', 'empty_heads.pt')))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path, config):\n",
    "        if not os.path.isfile(os.path.join(input_path, 'empty_heads_config.json')):\n",
    "            raise ValueError(\n",
    "                \"In the model path does not find empty_heads_config.json file, you may have not trained yet\")\n",
    "\n",
    "        with open(os.path.join(input_path, 'empty_heads_config.json')) as f:\n",
    "            config = json.load(f)\n",
    "        model = EmptyHeads()\n",
    "\n",
    "        if not os.path.isfile(os.path.join(input_path, 'empty_heads.pt')):\n",
    "            raise ValueError(\"In the model path does not find state of file, you need to train and get weights first\")\n",
    "\n",
    "        model.load_state_dict(torch.load(os.path.join(input_path, 'empty_heads.pt')))\n",
    "        return model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9b5cf7bd179003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第二步：定义评估器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0f856485fe75a0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=logging.BASIC_FORMAT,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SingleCLMEvaluator():\n",
    "    def __init__(self, dataloader: DataLoader = None,\n",
    "                 data_tag: str = \"dev\",\n",
    "                 device: int = None, tokenizer=None, early_stop_on: str = \"perplexity\"):\n",
    "\n",
    "        if data_tag not in [\"dev\", \"train\", \"test\"]:\n",
    "            raise ValueError(\"data_tag has to be one of dev, train or test\")\n",
    "        assert early_stop_on in [\"loss\", \"perplexity\"]\n",
    "        self.early_stop_on = early_stop_on\n",
    "        self.dataloader = dataloader\n",
    "        self.data_tag = data_tag\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.n_gpu = torch.cuda.device_count()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if device == -1:\n",
    "            self.n_gpu = 0\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def reset_dataloader(self, dataloader: DataLoader):\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "    def reset_logger(self, output_path):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model, collate_fn, output_path: str = None, epoch: int = -1, steps: int = -1,\n",
    "                 target_names: List[str] = None, do_predict: bool = False) -> Dict[\n",
    "        str, float]:\n",
    "\n",
    "        if do_predict and self.tokenizer == None:\n",
    "            raise ValueError(\"you are doing predict so need a tokenizer\")\n",
    "        if self.dataloader is None:\n",
    "            raise ValueError(\" need to set dataloader for this evaluator, call reset_dataloader()\")\n",
    "\n",
    "        model.eval()\n",
    "        if epoch == -1 and steps == -1:\n",
    "            logger.info(\n",
    "                f\"\\nEvaluation the model on {self.data_tag} dataset\")\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"\\nEvaluation the model on \" + self.data_tag + \" dataset\" + f\" in epoch {epoch} after {steps} steps:\")\n",
    "\n",
    "        self.dataloader.collate_fn = collate_fn\n",
    "        total_loss = 0.0\n",
    "        total_steps = 0\n",
    "\n",
    "        for step, batch in enumerate(tqdm(self.dataloader, desc=\"evaluating\")):\n",
    "            input = batch[\"features\"]\n",
    "            # batch to device\n",
    "            for feature_name, ids in input.items():\n",
    "                input[feature_name] = ids.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(input)\n",
    "                loss = loss.mean()\n",
    "                total_loss += loss\n",
    "\n",
    "            total_steps += 1\n",
    "        eval_loss = total_loss / total_steps\n",
    "        eval_results = {\"loss\": eval_loss}\n",
    "\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss)).clone().detach()\n",
    "        eval_results[\"perplexity\"] = perplexity.mean().item()\n",
    "        return eval_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abf9480104f21566"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第三步：定义训练器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c43a929df84d8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from typing import Type, Dict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=logging.BASIC_FORMAT,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    wandb.ensure_configured()\n",
    "    if wandb.api.api_key is None:\n",
    "        _has_wandb = False\n",
    "        wandb.termwarn(\"W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\")\n",
    "    else:\n",
    "        _has_wandb = False if os.getenv(\"WANDB_DISABLED\") else True\n",
    "except ImportError:\n",
    "    _has_wandb = False\n",
    "\n",
    "\n",
    "def set_seed(seed, n_gpu):\n",
    "    logger.info(f\"   see seed for random, numpy and torch {seed}\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def print_model_state_dict(model):\n",
    "    for param_tensor in model.state_dict():\n",
    "        logger.info(f\"{param_tensor}\\t{model.state_dict()[param_tensor].size()}\")\n",
    "\n",
    "\n",
    "def print_optimizer_state_dict(optimizer):\n",
    "    for var_name in optimizer.state_dict():\n",
    "        logger.info(f\"{var_name}\\t{optimizer.state_dict()[var_name]}\")\n",
    "\n",
    "\n",
    "def count_params(model: torch.nn.Module, print_details: bool = False):\n",
    "    trainable_count = 0\n",
    "    total_count = 0\n",
    "    if isinstance(model, torch.nn.Sequential):\n",
    "        for index in model._modules:\n",
    "            if print_details:\n",
    "                print_model_state_dict(model._modules[index])\n",
    "                logger.info(model._modules[index])\n",
    "            trainable_count += sum(p.numel() for p in model._modules[index].parameters() if p.requires_grad)\n",
    "            total_count += sum(p.numel() for p in model._modules[index].parameters())\n",
    "    else:\n",
    "        if print_details:\n",
    "            print_model_state_dict(model)\n",
    "            logger.info(model)\n",
    "        total_count = sum(p.numel() for p in model.parameters())\n",
    "        trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f'  Total params: {total_count}')\n",
    "    logger.info(f'  Trainable params: {trainable_count}')\n",
    "    logger.info(f'  Non-trainable params: {total_count - trainable_count}')\n",
    "\n",
    "\n",
    "def batch_to_device(batch, device, keep_label=False):\n",
    "    features = batch['features']\n",
    "    if isinstance(features, dict):\n",
    "        for feature_name in features:\n",
    "            features[feature_name] = features[feature_name].to(device)\n",
    "    else:\n",
    "        for inx in range(len(features)):\n",
    "            for feature_name in features[inx]:\n",
    "                features[inx][feature_name] = features[inx][feature_name].to(device)\n",
    "\n",
    "    label_space = batch['labels']\n",
    "    if label_space == None:  # for tasks like lm, labels are none.\n",
    "        return features, None\n",
    "    if not keep_label:\n",
    "        labels = {\"label_space_\" + str(inx): label_space[inx].to(device) if torch.is_tensor(label_space[inx]) else\n",
    "        label_space[inx] for inx in range(len(label_space))}\n",
    "    else:\n",
    "        labels = label_space\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def is_wandb_available():\n",
    "    return _has_wandb\n",
    "\n",
    "\n",
    "class CollateFunction():\n",
    "    def __init__(self, up_model):\n",
    "        self.up_model = up_model\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if isinstance(batch[0], dict):\n",
    "            padded_features = self.up_model.padding_features(batch)\n",
    "            return {'features': padded_features,\n",
    "                    \"labels\": None}  # label_ids are in features, this task does not need labels, we set\n",
    "\n",
    "\n",
    "class ModelTrainer():\n",
    "    def __init__(self, up_model: nn.Module, down_layer: nn.Module = None, train_dataset=None,\n",
    "                 dev_dataset=None, dev_evaluator=None,\n",
    "                 epochs: int = 1,\n",
    "                 visiable_device: str = \"0\",\n",
    "                 scheduler: str = 'warmuplinear',\n",
    "                 warmup_ratio: float = 0.1,\n",
    "                 optimizer_class: Type[Optimizer] = transformers.AdamW,\n",
    "                 optimizer_params: Dict[str, object] = {'lr': 5e-5, 'eps': 1e-6, 'correct_bias': False},\n",
    "                 weight_decay: float = 0.01,\n",
    "                 early_stop: int = 20,\n",
    "                 # 20 evaluation steps without improving on the early_stop_on metric as specified in dev_evaluator\n",
    "                 evaluation_steps: int = 500,\n",
    "                 output_path: str = None,\n",
    "                 save_best_model: bool = True,\n",
    "                 max_grad_norm: float = 1,\n",
    "                 fp16: bool = False,\n",
    "                 accumulation_steps=1,\n",
    "                 fp16_opt_level: str = 'O1',\n",
    "                 seed: int = 122,\n",
    "                 data_loader_shuffle=True,\n",
    "                 device: str = None,\n",
    "                 dev_batch_size: int = -1,  # the same as train_batch_size\n",
    "                 n_gpu: int = None,\n",
    "                 report_model: bool = True,\n",
    "                 per_gpu_train_batch_size: int = 8,\n",
    "                 restore_training: bool = False,\n",
    "                 local_rank: int = -1,\n",
    "                 wandb_config=None):\n",
    "        \"\"\"\n",
    "        this trainer is written for training a sequential model that contains an upstream_layer (usually transformers)\n",
    "        and a downstream_layer (usually task-specific heads like FF, RNN, CNN for encoding the output of upstram_layer)\n",
    "\n",
    "        :param up_model: transformers like transformers.GPT2LMHeadModel or transformers.BERTModel\n",
    "        :param down_layer: None if up_model already wraps up with an output encoder such as LMHead in GPT2LMHeadModel, else nn.Module for encoding the output of up_model\n",
    "        :param train_dataset: train_dataset, it can be either instance of torch.data.Dataset or IterableDataset (defined in data.py)\n",
    "        :param dev_dataset: dev_dataset, it can be either instance of torch.data.Dataset or IterableDataset\n",
    "        :param dev_evaluator: dev_evaluator, evaluator on dev_dataset for early stop and performance tracking during training (defined in evaluate.py)\n",
    "        :param epochs: number of epoches for training\n",
    "        :param visiable_device: devices chosen to perform training\n",
    "        :param scheduler: scheduler specially from transformers: see options in self._get_scheduler\n",
    "        :param warmup_ratio: warmup_ratio ratio for learning rate over total training steps\n",
    "        :param optimizer_class: transformers.AdamW de byfault\n",
    "        :param optimizer_params: optimizer params\n",
    "        :param weight_decay:weight decay\n",
    "        :param early_stop:early stop steps\n",
    "        :param evaluation_steps:logging steps\n",
    "        :param output_path: path to save the checkpoint with the best performance as specified in early_stop_on in dev_evaluator instance\n",
    "        :param save_best_model:save best checkpoint or the latest checkpoint\n",
    "        :param max_grad_norm:max grad norm\n",
    "        :param fp16: fp16 training\n",
    "        :param accumulation_steps:accumulation steps\n",
    "        :param fp16_opt_level:fp16 opt level\n",
    "        :param seed:random seed for reproducibility\n",
    "        :param data_loader_shuffle:Whether to shuffle data_loader of training dataset and dev dataset after epoch ends\n",
    "        :param device: device for training, None or gpu for gpu training, cpu for gpu training\n",
    "        :param dev_batch_size: development batch size, usually larger than training batch size due to no grads calculation and hence less burden on memory\n",
    "        :param n_gpu: number of gpus for training\n",
    "        :param report_model:if report model's structure and number of trainable params in logging\n",
    "        :param per_gpu_train_batch_size: what it means literally\n",
    "        :param restore_training: if restore training if the training process is interupped due to some accidents\n",
    "        :param local_rank:for distributed training\n",
    "        :param wandb_config: wandb logging if not none, else without wandb logging\n",
    "        \"\"\"\n",
    "\n",
    "        self.up_model = up_model\n",
    "        if down_layer == None:\n",
    "            # In this example, the upstream_layer already integrate the downstream head (namely, simple LM head as in transformers.GPT2LMHeadModel)\n",
    "            # EmptyHeads is created here only for placeholder purpose\n",
    "            down_layer = EmptyHeads()\n",
    "\n",
    "        self.down_layer = down_layer\n",
    "        assert output_path != None\n",
    "        output_path = os.path.join(\"tmp\", output_path)\n",
    "        # os.makedirs(output_path,exist_ok=True)\n",
    "        if restore_training:\n",
    "            if not os.listdir(output_path):\n",
    "                raise ValueError(f\"no checkpoint found in {output_path}\")\n",
    "            else:\n",
    "                logger.info(\"   loading embedding weights from saved checkpoint\")\n",
    "                self.up_model = self.up_model.reload(\n",
    "                    output_path)  # for other transformers (apart from bert), the load_saved function has not been added\n",
    "\n",
    "                logger.info(\"   loading downstream weights from saved checkpoint\")\n",
    "                self.down_layer.load_saved(output_path)\n",
    "                with open(output_path + \"/ck_report.json\") as f:\n",
    "                    self.ck_report = json.load(f)\n",
    "\n",
    "        self.model = torch.nn.Sequential(self.up_model, self.down_layer)\n",
    "\n",
    "        if is_wandb_available() and wandb_config != None:\n",
    "            # keep track of model topology and gradients if is_wandb_available and args!=None\n",
    "            wandb.init(project=wandb_config.wandb_project_name, config=wandb_config, name=wandb_config.wandb_run_name)\n",
    "            wandb.watch(\n",
    "                (self.up_model, self.down_layer), log_freq=max(100, evaluation_steps)\n",
    "            )\n",
    "        self.wandb_config = wandb_config\n",
    "\n",
    "        self._restore_training = restore_training\n",
    "        self.early_stop = early_stop\n",
    "\n",
    "        self._dev_evaluator = dev_evaluator\n",
    "\n",
    "        self._evaluation_steps = evaluation_steps\n",
    "        self._save_best_model = save_best_model\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        if os.listdir(output_path) and not restore_training:\n",
    "            # out = input(\n",
    "            #     \"Output directory ({}) already exists and is not empty, you wanna remove it before start? (y/n)\".format(\n",
    "            #         output_path))\n",
    "            # if out == \"y\":\n",
    "            #     shutil.rmtree(output_path)\n",
    "            #     os.makedirs(output_path, exist_ok=True)\n",
    "            # else:\n",
    "            #     raise ValueError(\"Output directory ({}) already exists and is not empty\".format(\n",
    "            #         output_path))\n",
    "            shutil.rmtree(output_path)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        logFormatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "        fileHandler = logging.FileHandler(os.path.join(output_path, \"log.out\"), mode=\"a\")\n",
    "        fileHandler.setFormatter(logFormatter)\n",
    "        logger.addHandler(fileHandler)\n",
    "        self._dev_evaluator.reset_logger(output_path)\n",
    "\n",
    "        self.output_path = output_path\n",
    "\n",
    "        if device is None or device == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                n_gpu = 1 if n_gpu == 1 else torch.cuda.device_count()\n",
    "            else:\n",
    "                logger.warning(\"no cuda is found in your machine, now use cpu\")\n",
    "                device = torch.device(\"cpu\")\n",
    "                n_gpu = 0\n",
    "        elif device == \"cpu\":\n",
    "            device = torch.device(\"cpu\")\n",
    "            n_gpu = 0\n",
    "        else:\n",
    "            raise ValueError(\"set device to be None, cuda or cpu\")\n",
    "        assert n_gpu <= torch.cuda.device_count()\n",
    "\n",
    "        logger.info(\"Use pytorch device: {}, with gpu_number={}\".format(device, n_gpu))\n",
    "\n",
    "        self._train_batch_size = per_gpu_train_batch_size * max(1, n_gpu)\n",
    "        self._dev_batch_size = dev_batch_size if dev_batch_size != -1 else self._train_batch_size\n",
    "\n",
    "        if isinstance(train_dataset, data.IterableDataset):\n",
    "            self._train_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "            self._steps_per_epoch = len(self._train_dataloader.dataset)\n",
    "        else:\n",
    "            self._train_dataloader = DataLoader(train_dataset, shuffle=data_loader_shuffle,\n",
    "                                                batch_size=self._train_batch_size)\n",
    "            self._steps_per_epoch = len(self._train_dataloader)\n",
    "\n",
    "        if isinstance(dev_dataset, data.IterableDataset):\n",
    "            dev_dataloader = DataLoader(dev_dataset, batch_size=None)\n",
    "        else:\n",
    "            dev_dataloader = DataLoader(dev_dataset, shuffle=data_loader_shuffle, batch_size=self._dev_batch_size)\n",
    "\n",
    "        if accumulation_steps > 1:\n",
    "            self._steps_per_epoch = self._steps_per_epoch // accumulation_steps\n",
    "\n",
    "        self._dev_data = dev_dataset\n",
    "        self._dev_evaluator.reset_dataloader(dev_dataloader)\n",
    "\n",
    "        self.collate_fn = CollateFunction(self.up_model)\n",
    "        # Use customize batching\n",
    "        self._train_dataloader.collate_fn = self.collate_fn\n",
    "\n",
    "        self._train_data = train_dataset\n",
    "        self._per_gpu_train_batch_size = per_gpu_train_batch_size\n",
    "\n",
    "        set_seed(seed, n_gpu)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=[int(i) for i in visiable_device.split(',')])\n",
    "            self.model = self.model.to(f'cuda:{self.model.device_ids[0]}')\n",
    "\n",
    "        elif n_gpu == 1:\n",
    "            self.model = self.model.to(device)\n",
    "\n",
    "        self._device = device\n",
    "        self._n_gpu = n_gpu\n",
    "\n",
    "        self._total_train_steps = int(self._steps_per_epoch * epochs)\n",
    "        self._epochs = epochs\n",
    "\n",
    "        if report_model:\n",
    "            count_params(self.model, print_details=True)\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        if local_rank != -1:\n",
    "            self._total_train_steps = self._total_train_steps // torch.distributed.get_world_size()\n",
    "\n",
    "        self._optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
    "\n",
    "        warmup_steps = math.ceil(self._total_train_steps * warmup_ratio)  # by default 20% of train data for warm-up\n",
    "        logger.info(f\"   Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "        self._scheduler = self._get_scheduler(self._optimizer, scheduler=scheduler, warmup_steps=warmup_steps,\n",
    "                                              num_total=self._total_train_steps)\n",
    "\n",
    "        if fp16:\n",
    "            try:\n",
    "                from apex import amp\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "\n",
    "            model, optimizer = amp.initialize(self.model, self._optimizer, opt_level=fp16_opt_level)\n",
    "            self.model = model\n",
    "            self._optimizer = optimizer\n",
    "\n",
    "        self._fp16 = fp16\n",
    "        tb_writer = None\n",
    "        if local_rank in [-1, 0]:\n",
    "            tb_writer = SummaryWriter()\n",
    "        self._tb_writer = tb_writer\n",
    "        self._local_rank = local_rank\n",
    "        self._best_score = -float(\"inf\")\n",
    "        self._early_stop_count = 0\n",
    "        self.last_time = datetime.now()\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        # assert evaluation_steps % accumulation_steps == 0, \"evaluation_steps should be divisable by accumulation_steps\"\n",
    "\n",
    "    def _train_epoch(self, epoch: int, global_steps: int):\n",
    "        epoch_steps = 0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        # 首先，初始化epoch的步数（epoch_steps）和损失（epoch_loss）为0，并清空模型的梯度（self.model.zero_grad()）。\n",
    "\n",
    "        for step, data in enumerate(\n",
    "                tqdm(self._train_dataloader, desc=\"training\", total=self._steps_per_epoch * self.accumulation_steps)\n",
    "        ):\n",
    "            # 然后，对训练数据进行迭代，使用enumerate函数遍历数据加载器（self._train_dataloader）。\n",
    "            # tqdm用于在终端显示进度条，desc参数设置进度条的描述为\"training\"，\n",
    "            # total参数设置总的迭代次数为self._steps_per_epoch * self.accumulation_steps。\n",
    "\n",
    "            self.model.train()\n",
    "            if data[\"labels\"] != \"skip-device\":\n",
    "                input, labels = batch_to_device(data, self._device)\n",
    "                # add labels to input for training where this step is ignored when inference\n",
    "                if isinstance(labels, dict):\n",
    "                    for idx in range(len(input)):\n",
    "                        input[idx].update(labels)\n",
    "            else:\n",
    "                input = data[\"features\"]\n",
    "            # 在每个步骤中，将模型设置为训练模式（self.model.train()）。\n",
    "            # 然后，检查数据中是否存在标签（data[\"labels\"] != \"skip-device\"）。\n",
    "            # 如果存在标签，则将输入数据（input）和标签数据（labels）移到指定设备（self._device）。\n",
    "            # 如果标签是一个字典，则将标签添加到输入数据中。\n",
    "            # 如果数据中没有标签，则将输入数据设置为data[\"features\"]。\n",
    "\n",
    "            loss_value, _ = self.model(input)\n",
    "\n",
    "            if self._n_gpu > 1:\n",
    "                loss_value = loss_value.mean()\n",
    "            if self.accumulation_steps > 1:\n",
    "                loss_value = loss_value / self.accumulation_steps\n",
    "            # 计算模型对输入数据的输出（loss_value）。\n",
    "            # 如果使用多个GPU进行训练（self._n_gpu > 1），则对损失值进行平均。\n",
    "            # 如果使用梯度累积（self.accumulation_steps > 1），则将损失值除以累积步数。\n",
    "\n",
    "            if self._fp16:\n",
    "                try:\n",
    "                    from apex import amp\n",
    "                except ImportError:\n",
    "                    raise ImportError(\n",
    "                        \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "                with amp.scale_loss(loss_value, self._optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(self._optimizer), self._max_grad_norm)\n",
    "            else:\n",
    "                loss_value.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self._max_grad_norm)\n",
    "            epoch_loss += loss_value\n",
    "            # 根据是否启用混合精度训练（self.fp16），计算并反向传播损失值（loss_value）。\n",
    "            # 如果启用混合精度训练，使用Apex库（需要安装）对损失值进行缩放（amp.scale_loss）和反向传播。\n",
    "            # 然后，使用梯度裁剪（torch.nn.utils.clip_grad_norm）限制梯度的大小，以避免梯度爆炸。\n",
    "            # 最后，将损失值添加到epoch_loss中。\n",
    "\n",
    "            if (step + 1) % self.accumulation_steps == 0:\n",
    "                self._optimizer.step()\n",
    "                self._scheduler.step()\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                epoch_steps += 1\n",
    "                total_global = epoch_steps + global_steps\n",
    "                # 如果达到了累积步数（self.accumulation_steps），则进行梯度更新。\n",
    "                # 调用优化器的step方法（self._optimizer.step()）执行梯度更新，并调用学习率调度器的step方法（self._scheduler.step()）更新学习率。\n",
    "                # 然后清空模型的梯度（self.model.zero_grad()）。增加epoch步数（epoch_steps），计算总的全局步数（total_global）。\n",
    "\n",
    "                if self._evaluation_steps > 0 and (total_global) % self._evaluation_steps == 0:\n",
    "                    # 如果设置了评估步数（self._evaluation_steps > 0），并且当前步数是评估步数的倍数，则进行模型评估。\n",
    "\n",
    "                    dev_loss, eval_scores = self._dev_eval_in_training(epoch, epoch_steps)\n",
    "                    logger.info(\"   ***** Evaluation report *****\")\n",
    "                    logger.info(f\"  Output path (short): {self.output_path}\")\n",
    "                    logger.info(f\"  Early stop on: {self._dev_evaluator.early_stop_on}\")\n",
    "                    logger.info(f\"  Early stop count = {self._early_stop_count}/{self.early_stop}\")\n",
    "                    logger.info(\n",
    "                        f\"  Eval steps = {self._evaluation_steps} or (iterations = {self._evaluation_steps * self.accumulation_steps})\")\n",
    "                    logger.info(f\"  Best score ({self._dev_evaluator.early_stop_on}) = {self._best_score}\")\n",
    "                    logger.info(f\"  Gradient Accumulation steps = {self.accumulation_steps}\")\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"  Num of training examples (actually no. of iterations per epoch for Iterable Dataset)  = {len(self._train_data)}\")\n",
    "                    logger.info(\n",
    "                        f\"  Num of development examples (actually no. of iterations per epoch for Iterable Dataset) = {len(self._dev_data)}\")\n",
    "                    now_time = datetime.now()\n",
    "                    logger.info(f\"  Time spent since last evaluation = {self.time_diff(self.last_time, now_time)}\")\n",
    "                    self.last_time = now_time\n",
    "\n",
    "                    logger.info(f\"  Epoch = {epoch + 1}/{self._epochs}\")\n",
    "                    logger.info(f\"  Steps = {total_global}/{self._total_train_steps}\")\n",
    "                    logger.info(\n",
    "                        f\"  Instantaneous batch size per GPU = {self._per_gpu_train_batch_size} and n_gpu = {self._n_gpu} so the input batch size = {self._train_batch_size}\")\n",
    "                    if dev_loss != None:\n",
    "                        logger.info(f\"  dev_loss = {dev_loss:.6f}\\t||\\t dev_eval_scores = {eval_scores}\")\n",
    "                    else:\n",
    "                        logger.info(f\"  dev_eval_scores = {eval_scores}\")\n",
    "                    # 进行模型评估，并获取开发集的损失（dev_loss）和评估指标（eval_scores）。然后，打印评估报告的各种信息，如输出路径、早停策略、评估步数、最佳评估指标分数等。\n",
    "\n",
    "                    train_loss = epoch_loss / epoch_steps\n",
    "                    logger.info(f\"  train_loss = {train_loss}\")\n",
    "                    logger.info(\"\\n********************************************\")\n",
    "\n",
    "                    if is_wandb_available() and self.wandb_config != None:\n",
    "                        if dev_loss != None:\n",
    "                            wandb.log(\n",
    "                                {\"loss_dev\": dev_loss,\n",
    "                                 f\"best_score_for_{self._dev_evaluator.early_stop_on}\": self._best_score,\n",
    "                                 \"loss_train\": train_loss, \"lr\": self._scheduler.get_lr()[0]},\n",
    "                                step=total_global)\n",
    "                        else:\n",
    "                            wandb.log({\"loss_train\": train_loss,\n",
    "                                       f\"best_score_for_{self._dev_evaluator.early_stop_on}\": self._best_score,\n",
    "                                       \"lr\": self._scheduler.get_lr()[0]},\n",
    "                                      step=total_global)\n",
    "                    # 计算平均训练损失（train_loss），并将其记录到日志中。\n",
    "                    # 如果使用了wandb（Weights & Biases）库，并且配置了wandb_config，则将训练损失、开发集损失、最佳评估指标分数和学习率记录到wandb中。\n",
    "\n",
    "                    for key, value in eval_scores.items():\n",
    "                        if is_wandb_available() and self.wandb_config != None:\n",
    "                            wandb.log({f\"eval_{key}_dev\": value}, step=total_global)\n",
    "                        self._tb_writer.add_scalar(f\"eval_{key}_dev\", value, total_global)\n",
    "\n",
    "                    self._tb_writer.add_scalar(\"lr\", self._scheduler.get_lr()[0], total_global)\n",
    "                    if dev_loss != None:\n",
    "                        self._tb_writer.add_scalar(\"loss_dev\", dev_loss, total_global)\n",
    "\n",
    "                    self._tb_writer.add_scalar(\"loss_train\", train_loss, total_global)\n",
    "\n",
    "                    if self._early_stop_count >= self.early_stop:\n",
    "                        logger.info(\n",
    "                            f\"  Continuous {self.early_stop} evaluation steps without loss reduction, so early stopped...\")\n",
    "                        sys.exit(0)\n",
    "            # 将评估指标的值记录到TensorBoard中，使用TensorBoard写入器（self._tb_writer）。\n",
    "            # 同时，将学习率、开发集损失和训练损失记录到TensorBoard中。\n",
    "            # 达到早停策略中设置的停止次数（self._early_stop_count >= self.early_stop），即开发集的损失在连续的评估步骤中没有减少，就会触发早停策略。\n",
    "            # 在这种情况下，程序记录日志信息，指示连续的评估步骤没有减少损失，并使用sys.exit(0)退出程序。\n",
    "\n",
    "        return epoch_loss, epoch_steps\n",
    "\n",
    "    def train(self):\n",
    "        if self._restore_training:\n",
    "            logger.info(f\"***** restoring training from the previous checkpoint: {self.ck_report}*****\")\n",
    "        else:\n",
    "            logger.info(\"***** Running training *****\")\n",
    "        logger.info(\n",
    "            f\"  Num of training examples (actually iterations per epoch for Iterable Dataset) = {len(self._train_data)}\")\n",
    "        logger.info(f\"  Output path (short): {self.output_path}\")\n",
    "        logger.info(\n",
    "            f\"  Steps per Epoch = {self._steps_per_epoch} or iterations per epoch = {self._steps_per_epoch * self.accumulation_steps}\")\n",
    "        logger.info(f\"  Num of Epochs = {self._epochs}\")\n",
    "        logger.info(f\"  Best score ({self._dev_evaluator.early_stop_on}) = {self._best_score}\")\n",
    "        logger.info(\n",
    "            f\"  Eval every {self._evaluation_steps} steps or every {self._evaluation_steps * self.accumulation_steps} iterations\")\n",
    "        logger.info(f\"  Early stop = {self.early_stop}\")\n",
    "        logger.info(f\"  Gradient Accumulation steps = {self.accumulation_steps}\")\n",
    "\n",
    "        logger.info(f\"  Total optimization steps = {self._total_train_steps}\")\n",
    "        logger.info(\n",
    "            f\"  Instantaneous batch size per GPU = {self._per_gpu_train_batch_size} and n_gpu = {self._n_gpu} so the input batch size = {self._train_batch_size}\")\n",
    "        global_loss = 0.0\n",
    "        global_steps = 0\n",
    "        self.last_time = datetime.now()\n",
    "        for epoch in trange(self._epochs, desc=\"Epoch\"):\n",
    "            epoch_loss, epoch_steps = self._train_epoch(epoch, global_steps)\n",
    "            global_loss += epoch_loss\n",
    "            global_steps += epoch_steps\n",
    "            logger.info(f\"epoch {epoch + 1} ends, {self._epochs - epoch - 1} epoches left\")\n",
    "            logger.info(\n",
    "                f\"\\nglobal_average_loss={global_loss / global_steps},global_steps={global_steps} on training set\")\n",
    "\n",
    "        if self._local_rank in [-1, 0]:\n",
    "            self._tb_writer.close()\n",
    "\n",
    "    def _dev_eval_in_training(self, epoch, steps):\n",
    "        return_scores = {}\n",
    "        if self._dev_evaluator is not None:\n",
    "\n",
    "            return_scores = self._dev_evaluator(self.model, self.collate_fn,\n",
    "                                                output_path=self.output_path, epoch=epoch, steps=steps)\n",
    "\n",
    "            early_stop_on = self._dev_evaluator.early_stop_on\n",
    "\n",
    "            check_score = -return_scores[early_stop_on] if early_stop_on == \"loss\" or early_stop_on == \"perplexity\" else \\\n",
    "                return_scores[early_stop_on]\n",
    "            if check_score >= self._best_score and self._save_best_model:\n",
    "                eval_scores_transformed = {key:\n",
    "                                               return_scores[key].item() if torch.is_tensor(return_scores[key]) else\n",
    "                                               return_scores[key]\n",
    "                                           for key in return_scores.keys()}\n",
    "                self.save(self.output_path,\n",
    "                          {\"training_examples (when pos_num=1 for ranking)\": len(self._train_data),\n",
    "                           \"evaluation_steps\": self._evaluation_steps,\n",
    "                           \"train_batch_size\": self._train_batch_size, \"epoch\": epoch + 1, \"total_epochs\": self._epochs,\n",
    "                           \"steps\": steps,\n",
    "                           \"saved_at_total_steps\": steps + epoch * self._steps_per_epoch,\n",
    "                           \"steps_per_epoch\": self._steps_per_epoch, \"eval_scores_on_dev\": eval_scores_transformed})\n",
    "\n",
    "                self._best_score = check_score\n",
    "\n",
    "                logger.info(f\"  Save check-point at epoch={epoch} step={steps}\")\n",
    "                self._early_stop_count = 0\n",
    "            else:\n",
    "                self._early_stop_count += 1\n",
    "\n",
    "        return return_scores.pop(\"loss\").item() if \"loss\" in return_scores else None, return_scores\n",
    "\n",
    "    def save(self, path, eval_details):\n",
    "        if path is None:\n",
    "            return\n",
    "        logger.info(f\"   Save model to {path}\")\n",
    "        contained_modules = []\n",
    "\n",
    "        to_iterate = self.model.module._modules if self._n_gpu > 1 else self.model._modules\n",
    "\n",
    "        for idx, name in enumerate(to_iterate):\n",
    "            module = to_iterate[str(name)]\n",
    "\n",
    "            model_path = os.path.join(path, str(idx) + \"_\" + type(module).__name__)\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            module.save(model_path)\n",
    "            contained_modules.append(\n",
    "                {'idx': idx, 'name': name, 'path': os.path.basename(model_path), 'type': type(module).__module__})\n",
    "\n",
    "        if self.wandb_config != None:\n",
    "            with open(os.path.join(path, 'hyperparams.json'), 'w') as f:\n",
    "                json.dump(self.wandb_config.__dict__, f, indent=2)\n",
    "\n",
    "        with open(os.path.join(path, 'modules.json'), 'w') as fOut:\n",
    "            json.dump(contained_modules, fOut, indent=2)\n",
    "        with open(os.path.join(path, 'ck_report.json'), 'w') as fOut:\n",
    "            json.dump(eval_details, fOut, indent=2)\n",
    "\n",
    "    def _get_scheduler(self, optimizer, scheduler: str, warmup_steps: int, num_total: int):\n",
    "        assert scheduler in [\"constantlr\", \"warmuplinear\", \"warmupconstant\", \"warmupcosine\",\n",
    "                             \"warmupcosinewithhardrestarts\"], (\n",
    "            'scheduler should be one of [\"constantlr\",\"warmupconstant\",\"warmupcosine\",\"warmupcosinewithhardrestarts\"]')\n",
    "        if scheduler == 'constantlr':\n",
    "            return transformers.get_constant_schedule(optimizer)\n",
    "        elif scheduler == 'warmupconstant':\n",
    "            return transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "        elif scheduler == 'warmuplinear':\n",
    "            return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                                num_training_steps=num_total)\n",
    "        elif scheduler == 'warmupcosine':\n",
    "            return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                                num_training_steps=num_total)\n",
    "        elif scheduler == 'warmupcosinewithhardrestarts':\n",
    "            return transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
    "                                                                                   num_warmup_steps=warmup_steps,\n",
    "                                                                                   num_training_steps=num_total)\n",
    "\n",
    "    def time_diff(self, t_a, t_b):\n",
    "        t_diff = relativedelta(t_b, t_a)  # later/end time comes first!\n",
    "        return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7974349eab7c5b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第四步：定义数据集处理模块"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e18704ed4c0563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SrcCodeDataset(Dataset):\n",
    "    def __init__(self, file_path, model, cache_path=None):\n",
    "        \"\"\"\n",
    "        this dataset class is used to load source code dataset in batch for fine-tuning with GPT2LMModel\n",
    "        :param model: the model that the dataset will be fed to\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        load_cache = False\n",
    "        if cache_path != None:\n",
    "            load_cache = self._load_cache(cache_path)\n",
    "        if not load_cache:\n",
    "            self._build(file_path, model)\n",
    "        if cache_path != None:\n",
    "            self._cache(cache_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[index][\"input_ids\"]\n",
    "        # input_mask = self.inputs[index][\"attention_mask\"] we don't need attention_mask for this task\n",
    "        # return {\"input_ids\": input_ids, \"input_mask\": input_mask}\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "    def _load_cache(self, cache_path):\n",
    "        load_cache = False\n",
    "        if os.path.isdir(cache_path):\n",
    "            if os.path.isfile(os.path.join(cache_path, \"inputs.pk\")):\n",
    "                with open(os.path.join(cache_path, \"inputs.pk\"), \"rb\") as f:\n",
    "                    logger.info(\n",
    "                        f\"  load cached token ids of model from {cache_path}\")\n",
    "                    self.inputs = pickle.load(f)\n",
    "                    load_cache = True\n",
    "        return load_cache\n",
    "\n",
    "    def _cache(self, cache_path):\n",
    "        if not os.path.isdir(cache_path):\n",
    "            os.makedirs(cache_path)\n",
    "        with open(os.path.join(cache_path, \"inputs.pk\"), \"wb\") as f:\n",
    "            pickle.dump(self.inputs, f)\n",
    "            logger.info(\n",
    "                f\"  save tokenized ids of samples to: {cache_path}/inputs.pk\")\n",
    "\n",
    "    def _build(self, file_path, model):\n",
    "        with open(file_path) as f:\n",
    "            for line in tqdm(f):\n",
    "                example = json.loads(line.strip())\n",
    "                if example[\"label\"].lower() == \"python\":\n",
    "                    encoded_plus = model.tokenizer.encode_plus(\n",
    "                        model.tokenize(\"<python>\") + example[\"token_ids\"] + [model.eos_token_id],\n",
    "                        max_length=model.max_seq_length)\n",
    "                elif example[\"label\"].lower() == \"java\":\n",
    "                    encoded_plus = model.tokenizer.encode_plus(\n",
    "                        model.tokenize(\"<java>\") + example[\"token_ids\"] + [model.eos_token_id],\n",
    "                        max_length=model.max_seq_length)\n",
    "                self.inputs.append(encoded_plus.data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83874f56e324ff12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第五步：开始训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede9103b8e3d705f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77926211cac05016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=logging.BASIC_FORMAT,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dic = dict(model_select='distilgpt2',\n",
    "           # model_select='/kaggle/input/distilgpt2-model/distilgpt2',\n",
    "           dataset_name='source_code',\n",
    "           per_gpu_train_batch_size=4,\n",
    "           dev_batch_size=8,\n",
    "\n",
    "           num_epochs_train=10,\n",
    "           max_seq_length=256,\n",
    "           lr=2e-05,\n",
    "           warmup_ratio=0.2,\n",
    "\n",
    "           early_stop=20,\n",
    "           scheduler='warmuplinear',\n",
    "           seed=122,\n",
    "           accumulation_steps=1,\n",
    "           n_gpu=1,\n",
    "           visiable_device='0',\n",
    "           evaluation_steps=200,\n",
    "           wandb_project_name='code_generate',\n",
    "           restore_training=False,\n",
    "           with_wandb=True)\n",
    "\n",
    "args = argparse.Namespace(**dic)\n",
    "logger.info(f\"args: {args}\")\n",
    "dataset_folder = f\"/content/drive/MyDrive/lab2/auto_coding-master/dataset/{args.dataset_name}/json/\"\n",
    "output_path = f\"/content/drive/MyDrive/lab2/auto_coding-master/model/{args.model_select}_fine_tuned_coder_1\"\n",
    "# dataset_folder = f\"/kaggle/input/distligpt2-data/source_code/json/\"\n",
    "# output_path = f\"/kaggle/working/model/distligpt2_fine_tuned_coder_1\"\n",
    "# initialize model by model name (the same as used in transformers lib)\n",
    "model = GPTSingleHead(args.model_select, max_seq_length=args.max_seq_length)\n",
    "# add special tokens for controlling code generation by different programming language\n",
    "model.add_special_words({\"pad_token\": \"<pad>\", \"additional_special_tokens\": [\"<python>\", \"<java>\"]})\n",
    "# load training dataset\n",
    "file_path = dataset_folder + \"train.jsonl\"\n",
    "train_dataset = SrcCodeDataset(file_path, model, cache_path=os.path.join(\".cache\", output_path, \"train\"))\n",
    "# load developlemt dataset\n",
    "file_path = dataset_folder + \"dev.jsonl\"\n",
    "dev_dataset = SrcCodeDataset(file_path, model, cache_path=os.path.join(\".cache\", output_path, \"dev\"))\n",
    "# initialize development evaluator\n",
    "dev_evaluator = SingleCLMEvaluator()\n",
    "# initialize model trainer\n",
    "model_trainer = ModelTrainer(model,\n",
    "                             train_dataset=train_dataset,\n",
    "                             dev_dataset=dev_dataset,\n",
    "                             dev_evaluator=dev_evaluator,\n",
    "                             scheduler=args.scheduler,\n",
    "                             epochs=args.num_epochs_train,\n",
    "                             per_gpu_train_batch_size=args.per_gpu_train_batch_size,\n",
    "                             output_path=output_path,\n",
    "                             optimizer_params={'lr': args.lr, 'eps': 1e-6, 'correct_bias': False},\n",
    "                             evaluation_steps=args.evaluation_steps,\n",
    "                             early_stop=args.early_stop,\n",
    "                             dev_batch_size=args.dev_batch_size,\n",
    "                             restore_training=args.restore_training,\n",
    "                             accumulation_steps=args.accumulation_steps,\n",
    "                             n_gpu=args.n_gpu,\n",
    "                             visiable_device=args.visiable_device,\n",
    "                             warmup_ratio=args.warmup_ratio,\n",
    "                             seed=args.seed,\n",
    "                             data_loader_shuffle=True,\n",
    "                             wandb_config=args if args.with_wandb else None)\n",
    "# start training\n",
    "model_trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2804c7b07b90c11d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
