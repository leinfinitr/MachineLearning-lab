{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7235532,
     "sourceType": "datasetVersion",
     "datasetId": 4189966
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install timm\n",
    "!pip install albumentaions\n",
    "!pip install loguru"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-19T06:30:11.909656Z",
     "iopub.execute_input": "2023-12-19T06:30:11.910071Z",
     "iopub.status.idle": "2023-12-19T06:30:42.824915Z",
     "shell.execute_reply.started": "2023-12-19T06:30:11.910036Z",
     "shell.execute_reply": "2023-12-19T06:30:42.823774Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.12)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.15.1+cpu)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.19.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.66.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.24.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n\u001B[31mERROR: Could not find a version that satisfies the requirement albumentaions (from versions: none)\u001B[0m\u001B[31m\n\u001B[0m\u001B[31mERROR: No matching distribution found for albumentaions\u001B[0m\u001B[31m\n\u001B[0mCollecting loguru\n  Obtaining dependency information for loguru from https://files.pythonhosted.org/packages/03/0a/4f6fed21aa246c6b49b561ca55facacc2a44b87d65b8b92362a8e99ba202/loguru-0.7.2-py3-none-any.whl.metadata\n  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\nDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import albumentations\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from loguru import logger"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-19T06:31:31.529959Z",
     "iopub.execute_input": "2023-12-19T06:31:31.530452Z",
     "iopub.status.idle": "2023-12-19T06:31:31.571437Z",
     "shell.execute_reply.started": "2023-12-19T06:31:31.530416Z",
     "shell.execute_reply": "2023-12-19T06:31:31.570055Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第一步：定义数据集处理函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    用于加载视频及其类标签的自定义数据集\n",
    "    :param data_dir: 数据集的路径\n",
    "    :param num_class: 数据集中视频的类别数\n",
    "    :param num_frame: 每个视频采样的帧数\n",
    "    :param transform: 数据预处理的转换函数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, num_class=10, num_frame=20, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_class\n",
    "        self.num_frames = num_frame\n",
    "        self.transform = transform\n",
    "\n",
    "        self.video_filename_list = []\n",
    "        self.classesIdx_list = []\n",
    "\n",
    "        self.class_dict = {class_label: idx for idx, class_label in enumerate(\n",
    "            sorted(os.listdir(self.data_dir)))}\n",
    "\n",
    "        for class_label, class_idx in self.class_dict.items():\n",
    "            class_dir = os.path.join(self.data_dir, class_label)\n",
    "            for video_filename in sorted(os.listdir(class_dir)):\n",
    "                self.video_filename_list.append(\n",
    "                    os.path.join(class_label, video_filename))\n",
    "                self.classesIdx_list.append(class_idx)\n",
    "\n",
    "    # 返回数据集中视频的数量\n",
    "    def __len__(self):\n",
    "        return len(self.video_filename_list)\n",
    "\n",
    "    # 读取视频文件，并进行帧的采样和数据预处理。返回采样后的帧序列。\n",
    "    def read_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv.VideoCapture(video_path)\n",
    "        count_frames = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                if self.transform:\n",
    "                    transformed = self.transform(image=frame)\n",
    "                    frame = transformed['image']\n",
    "\n",
    "                frames.append(frame)\n",
    "                count_frames += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        stride = count_frames // self.num_frames\n",
    "        new_frames = []\n",
    "        count = 0\n",
    "        for i in range(0, count_frames, stride):\n",
    "            if count >= self.num_frames:\n",
    "                break\n",
    "            new_frames.append(frames[i])\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        return torch.stack(new_frames, dim=0)\n",
    "\n",
    "    # 返回数据集中索引idx对应的视频及其类别标签\n",
    "    def __getitem__(self, idx):\n",
    "        classIdx = self.classesIdx_list[idx]\n",
    "        video_filename = self.video_filename_list[idx]\n",
    "        video_path = os.path.join(self.data_dir, video_filename)\n",
    "        frames = self.read_video(video_path)\n",
    "        return frames, classIdx"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-19T06:31:36.646272Z",
     "iopub.execute_input": "2023-12-19T06:31:36.646709Z",
     "iopub.status.idle": "2023-12-19T06:32:40.626219Z",
     "shell.execute_reply.started": "2023-12-19T06:31:36.646677Z",
     "shell.execute_reply": "2023-12-19T06:32:40.624725Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第二步：定义模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    \"\"\"\n",
    "    定义LSTM模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.Lstm = nn.LSTM(latent_dim, hidden_size, num_layers=lstm_layers, batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    # 重置LSTM的隐藏层状态\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, self.hidden_state = self.Lstm(x, self.hidden_state)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预训练CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PretrainedConv(nn.Module):\n",
    "    \"\"\"\n",
    "    使用预训练的ResNet152模型作为卷积层\n",
    "    :param latent_dim: 输出的特征维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PretrainedConv, self).__init__()\n",
    "        # 使用预训练的ResNet152模型\n",
    "        self.conv_model = torchvision.models.resnet152(pretrained=True)\n",
    "        # ====== 固定卷积层的参数 ======\n",
    "        for param in self.conv_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # ====== 修改最后一层全连接层 ======\n",
    "        # latent_dim为输出的特征维度，也是LSTM的输入维度\n",
    "        self.conv_model.fc = nn.Linear(self.conv_model.fc.in_features, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_model(x)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用预训练的CNN和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PretrainedConvLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用预训练的CNN和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(PretrainedConvLstm, self).__init__()\n",
    "        self.conv_model = PretrainedConv(latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size * time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的普通CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义的普通CNN模型\n",
    "    :param latent_dim: 输出的特征维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv_model = nn.Sequential(\n",
    "            # 输入维度：(batch_size, 3, 128, 128)\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=6, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 32, 32)\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 16, 16)\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=2), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 8, 8)\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 4, 4)\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        x = x.view(batch_size * time_steps, channel_x, height, width)\n",
    "        x = self.conv_model(x)\n",
    "        x = x.view(batch_size * time_steps, -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch_size, time_steps, -1)\n",
    "        return x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的CNN和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用自定义的CNN和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(ConvLstm, self).__init__()\n",
    "        self.conv_model = Conv(latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size, time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的ResNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义基本的残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet模型\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self.make_layer(64, 3)\n",
    "        self.layer2 = self.make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self.make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self.make_layer(512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用自定义的ResNet和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResNetLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用自定义的ResNet和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(ResNetLstm, self).__init__()\n",
    "        self.conv_model = ResNet(num_classes=latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size, time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第三步：定义评估函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义评估函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, val_data, loss_fn, device):\n",
    "    \"\"\"\n",
    "    评估模型在验证集上的性能\n",
    "    :param model: 评估的模型\n",
    "    :param val_data: 验证数据集的数据加载器\n",
    "    :param loss_fn: 损失函数\n",
    "    :param device: 训练设备\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    # 将模型移动到设备上（如GPU）\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 在评估阶段，关闭梯度计算\n",
    "        model.eval()\n",
    "\n",
    "        # 初始化变量用于计算准确率和损失\n",
    "        val_correct = 0\n",
    "        val_total = len(val_data) * val_data.batch_size\n",
    "        running_loss = 0.\n",
    "        # 使用 tqdm 进度条显示进度\n",
    "        val_data = tqdm(val_data, desc='Evaluate: ', ncols=100)\n",
    "\n",
    "        # 遍历验证数据集\n",
    "        for data_batch, label_batch in val_data:\n",
    "            data_batch, label_batch = data_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            # 前向传播计算输出\n",
    "            output_batch = model(data_batch)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = loss_fn(output_batch, label_batch.long())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 预测标签\n",
    "            _, predicted_labels = torch.max(output_batch.data, dim=1)\n",
    "\n",
    "            # 统计正确预测的数量\n",
    "            val_correct += (label_batch == predicted_labels).sum().item()\n",
    "\n",
    "        # 计算平均损失和准确率\n",
    "        val_loss = running_loss / len(val_data)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # 返回验证集的损失和准确率\n",
    "        return val_loss, val_acc"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义可视化函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_history(history):\n",
    "    \"\"\"\n",
    "    可视化训练过程中的损失和准确率\n",
    "    :param history: 训练过程中的损失和准确率\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='train_loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第四步：定义训练函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_data, loss_fn, optimizer, epochs, device, save_last_weights_path=None,\n",
    "          save_best_weights_path=None, steps_per_epoch=None,\n",
    "          validation_data=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param model: 要训练的模型。\n",
    "    :param train_data: 训练数据集的数据加载器。\n",
    "    :param loss_fn: 损失函数。\n",
    "    :param optimizer: 优化器。\n",
    "    :param epochs: 训练的轮数。\n",
    "    :param device: 训练设备。\n",
    "    :param save_last_weights_path: 可选参数，保存最后模型权重的路径。\n",
    "    :param save_best_weights_path: 可选参数，保存最佳模型权重的路径。\n",
    "    :param steps_per_epoch: 可选参数，每个epoch的步数。\n",
    "    :param validation_data: 可选参数，用于验证的数据加载器。\n",
    "    :param scheduler: 可选参数，学习率调度器。\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    if save_best_weights_path:\n",
    "        # 评估当前模型在验证数据集上的损失\n",
    "        best_loss, _ = evaluate(model, validation_data, loss_fn, device)\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        # 如果没有指定每个epoch的步数，则将其设置为训练数据集的长度\n",
    "        steps_per_epoch = len(train_data)\n",
    "\n",
    "    num_steps = len(train_data)\n",
    "    iterator = iter(train_data)\n",
    "    count_steps = 1\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    # 将模型移动到设备上\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 遍历每个epoch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        running_loss = 0.\n",
    "        train_correct = 0\n",
    "        train_total = steps_per_epoch * train_data.batch_size\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step in tqdm(range(steps_per_epoch), desc=f'epoch: {epoch}/{epochs}: ', ncols=100):\n",
    "            img_batch, label_batch = next(iterator)\n",
    "            img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n",
    "            # 将梯度置零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播计算输出\n",
    "            output_batch = model(img_batch)\n",
    "            # 计算损失\n",
    "            loss = loss_fn(output_batch, label_batch.long())\n",
    "            # 反向传播计算梯度\n",
    "            loss.backward(retain_graph=True)\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 预测标签\n",
    "            _, predicted_labels = torch.max(output_batch.data, dim=1)\n",
    "            # 统计正确预测的数量\n",
    "            train_correct += (label_batch == predicted_labels).sum().item()\n",
    "            # 计算平均损失\n",
    "            running_loss += loss.item()\n",
    "            # 打印训练损失和准确率\n",
    "            if count_steps == num_steps:\n",
    "                # 循环迭代器，以便继续训练数据集的下一个epoch\n",
    "                count_steps = 0\n",
    "                iterator = iter(train_data)\n",
    "            count_steps += 1\n",
    "\n",
    "        train_loss = running_loss / steps_per_epoch\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        if scheduler:\n",
    "            # 如果提供了学习率调度器，则根据训练损失调整学习率\n",
    "            scheduler.step(train_loss)\n",
    "\n",
    "        history['train_loss'].append(float(train_loss))\n",
    "        history['train_acc'].append(float(train_accuracy))\n",
    "\n",
    "        # 评估模型在验证数据集上的性能\n",
    "        val_loss, val_acc = evaluate(model, validation_data, loss_fn, device)\n",
    "        # 打印训练损失和准确率\n",
    "        print(\n",
    "            f'epoch: {epoch}, train_accuracy: {train_accuracy:.2f}, loss: {train_loss:.3f}, val_accuracy: {val_acc:.2f}, val_loss: {val_loss:.3f}')\n",
    "\n",
    "        if save_best_weights_path:\n",
    "            if val_loss < best_loss:\n",
    "                # 如果验证损失更小，则保存模型的权重\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_best_weights_path)\n",
    "                print(f'Saved successfully best weights to:', save_best_weights_path)\n",
    "        history['val_loss'].append(float(val_loss))\n",
    "        history['val_acc'].append(float(val_acc))\n",
    "\n",
    "    if save_last_weights_path:\n",
    "        # 如果提供了保存最后权重的路径，则保存模型的权重\n",
    "        torch.save(model.state_dict(), save_last_weights_path)\n",
    "        print(f'Saved successfully last weights to:', save_last_weights_path)\n",
    "\n",
    "    return model, history"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第五步：训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 指定模型参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 固定的参数\n",
    "num_classes = 10\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 可调整的参数\n",
    "num_frames = 15  # You can adjust this to balance speed and accuracy\n",
    "img_size = (128, 128)  # You can adjust this to balance speed and accuracy\n",
    "latent_dim = 2048\n",
    "hid_size = 128\n",
    "num_lstm_layers = 2\n",
    "learning_rate = 2e-5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数据预处理的转换流程。\n",
    "# 使用albumentations库进行图像处理，包括图像大小调整、归一化和转换为张量。\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(height=img_size[0], width=img_size[1]),\n",
    "        albumentations.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info('Loading dataset')\n",
    "# 加载数据集并指定数据集的路径、帧数、类别数和数据预处理的转换函数。\n",
    "full_dataset = VideoDataset(data_dir=\"/kaggle/input/ucf-101-dataset-extract-10/data\", num_class=num_classes,\n",
    "                            num_frame=num_frames, transform=transform)\n",
    "# 将数据集分为训练集和测试集，其中测试集的比例为0.2。\n",
    "train_dataset, test_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "# 使用PyTorch的DataLoader加载数据集。\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "logger.info('Dataset loaded')\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = PretrainedConvLstm(latent_dim=latent_dim, hidden_size=hid_size, lstm_layers=num_lstm_layers, bidirectional=True,\n",
    "                           n_class=num_classes)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, mode='min', patience=3, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history = train(model, train_loader, loss_fn, optimizer, epochs=30, device=device,\n",
    "                       save_last_weights_path='/kaggle/working/last_model.pth',\n",
    "                       save_best_weights_path='/kaggle/working/last_model.pth', validation_data=test_loader,\n",
    "                       scheduler=scheduler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第六步：评估模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_history(history)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, val_data=test_loader, loss_fn=loss_fn, device=device)\n",
    "print(f'Loss: {test_loss : .3f}, Acc: {test_acc: .3f}')\n",
    "\n",
    "test_loss, test_acc = evaluate(model, val_data=train_loader, loss_fn=loss_fn, device=device)\n",
    "print(f'Loss: {test_loss : .3f}, Acc: {test_acc: .3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
