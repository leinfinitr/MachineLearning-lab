{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7235532,
     "sourceType": "datasetVersion",
     "datasetId": 4189966
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install timm\n",
    "!pip install albumentaions\n",
    "!pip install loguru"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-19T06:30:11.909656Z",
     "iopub.execute_input": "2023-12-19T06:30:11.910071Z",
     "iopub.status.idle": "2023-12-19T06:30:42.824915Z",
     "shell.execute_reply.started": "2023-12-19T06:30:11.910036Z",
     "shell.execute_reply": "2023-12-19T06:30:42.823774Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T03:28:31.948745400Z",
     "start_time": "2023-12-20T03:28:26.280338600Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in d:\\program\\python\\lib\\site-packages (0.9.12)\n",
      "Requirement already satisfied: torch>=1.7 in d:\\program\\python\\lib\\site-packages (from timm) (2.3.0.dev20231218+cpu)\n",
      "Requirement already satisfied: torchvision in d:\\program\\python\\lib\\site-packages (from timm) (0.1.6)\n",
      "Requirement already satisfied: pyyaml in d:\\program\\python\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in d:\\program\\python\\lib\\site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: safetensors in d:\\program\\python\\lib\\site-packages (from timm) (0.4.1)\n",
      "Requirement already satisfied: filelock in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\program\\python\\lib\\site-packages (from torch>=1.7->timm) (2023.12.2)\n",
      "Requirement already satisfied: requests in d:\\program\\python\\lib\\site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\program\\python\\lib\\site-packages (from huggingface-hub->timm) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\program\\python\\lib\\site-packages (from huggingface-hub->timm) (23.2)\n",
      "Requirement already satisfied: colorama in d:\\program\\python\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program\\python\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program\\python\\lib\\site-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program\\python\\lib\\site-packages (from requests->huggingface-hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\program\\python\\lib\\site-packages (from requests->huggingface-hub->timm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program\\python\\lib\\site-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\program\\python\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement albumentaions (from versions: none)\n",
      "ERROR: No matching distribution found for albumentaions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in d:\\program\\python\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: colorama>=0.3.4 in d:\\program\\python\\lib\\site-packages (from loguru) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in d:\\program\\python\\lib\\site-packages (from loguru) (1.1.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import albumentations\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-19T06:31:31.529959Z",
     "iopub.execute_input": "2023-12-19T06:31:31.530452Z",
     "iopub.status.idle": "2023-12-19T06:31:31.571437Z",
     "shell.execute_reply.started": "2023-12-19T06:31:31.530416Z",
     "shell.execute_reply": "2023-12-19T06:31:31.570055Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T03:28:43.573891600Z",
     "start_time": "2023-12-20T03:28:40.982127500Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'functional' from 'torchvision.transforms' (D:\\Program\\python\\Lib\\site-packages\\torchvision\\transforms.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01malbumentations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ToTensorV2\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mloguru\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logger\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
      "File \u001B[1;32mD:\\Program\\python\\Lib\\site-packages\\albumentations\\pytorch\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mD:\\Program\\python\\Lib\\site-packages\\albumentations\\pytorch\\transforms.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms_interface\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BasicTransform\n\u001B[0;32m     11\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToTensorV2\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'functional' from 'torchvision.transforms' (D:\\Program\\python\\Lib\\site-packages\\torchvision\\transforms.py)"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第一步：定义数据集处理函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    用于加载视频及其类标签的自定义数据集\n",
    "    :param data_dir: 数据集的路径\n",
    "    :param num_class: 数据集中视频的类别数\n",
    "    :param num_frame: 每个视频采样的帧数\n",
    "    :param transform: 数据预处理的转换函数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, num_class=10, num_frame=20, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_class\n",
    "        self.num_frames = num_frame\n",
    "        self.transform = transform\n",
    "\n",
    "        self.video_filename_list = []\n",
    "        self.classesIdx_list = []\n",
    "\n",
    "        self.class_dict = {class_label: idx for idx, class_label in enumerate(\n",
    "            sorted(os.listdir(self.data_dir)))}\n",
    "\n",
    "        for class_label, class_idx in self.class_dict.items():\n",
    "            class_dir = os.path.join(self.data_dir, class_label)\n",
    "            for video_filename in sorted(os.listdir(class_dir)):\n",
    "                self.video_filename_list.append(\n",
    "                    os.path.join(class_label, video_filename))\n",
    "                self.classesIdx_list.append(class_idx)\n",
    "\n",
    "    # 返回数据集中视频的数量\n",
    "    def __len__(self):\n",
    "        return len(self.video_filename_list)\n",
    "\n",
    "    # 读取视频文件，并进行帧的采样和数据预处理。返回采样后的帧序列。\n",
    "    def read_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv.VideoCapture(video_path)\n",
    "        count_frames = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                if self.transform:\n",
    "                    transformed = self.transform(image=frame)\n",
    "                    frame = transformed['image']\n",
    "\n",
    "                frames.append(frame)\n",
    "                count_frames += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        stride = count_frames // self.num_frames\n",
    "        new_frames = []\n",
    "        count = 0\n",
    "        for i in range(0, count_frames, stride):\n",
    "            if count >= self.num_frames:\n",
    "                break\n",
    "            new_frames.append(frames[i])\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        return torch.stack(new_frames, dim=0)\n",
    "\n",
    "    # 返回数据集中索引idx对应的视频及其类别标签\n",
    "    def __getitem__(self, idx):\n",
    "        classIdx = self.classesIdx_list[idx]\n",
    "        video_filename = self.video_filename_list[idx]\n",
    "        video_path = os.path.join(self.data_dir, video_filename)\n",
    "        frames = self.read_video(video_path)\n",
    "        return frames, classIdx"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-19T06:31:36.646272Z",
     "iopub.execute_input": "2023-12-19T06:31:36.646709Z",
     "iopub.status.idle": "2023-12-19T06:32:40.626219Z",
     "shell.execute_reply.started": "2023-12-19T06:31:36.646677Z",
     "shell.execute_reply": "2023-12-19T06:32:40.624725Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第二步：定义模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLstm\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m    定义LSTM模型\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m    :param latent_dim: LSTM的输入维度\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m    :param bidirectional: LSTM是否为双向\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, latent_dim, hidden_size, lstm_layers, bidirectional):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Lstm(nn.Module):\n",
    "    \"\"\"\n",
    "    定义LSTM模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.Lstm = nn.LSTM(latent_dim, hidden_size, num_layers=lstm_layers, batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    # 重置LSTM的隐藏层状态\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, self.hidden_state = self.Lstm(x, self.hidden_state)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T02:55:40.345458300Z",
     "start_time": "2023-12-20T02:55:40.320368500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 预训练CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PretrainedConv(nn.Module):\n",
    "    \"\"\"\n",
    "    使用预训练的ResNet152模型作为卷积层\n",
    "    :param latent_dim: 输出的特征维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PretrainedConv, self).__init__()\n",
    "        # 使用预训练的ResNet152模型\n",
    "        self.conv_model = torchvision.models.resnet152(pretrained=True)\n",
    "        # ====== 固定卷积层的参数 ======\n",
    "        for param in self.conv_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # ====== 修改最后一层全连接层 ======\n",
    "        # latent_dim为输出的特征维度，也是LSTM的输入维度\n",
    "        self.conv_model.fc = nn.Linear(self.conv_model.fc.in_features, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_model(x)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用预训练的CNN和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PretrainedConvLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用预训练的CNN和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(PretrainedConvLstm, self).__init__()\n",
    "        self.conv_model = PretrainedConv(latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size * time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的普通CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义的普通CNN模型\n",
    "    :param latent_dim: 输出的特征维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv_model = nn.Sequential(\n",
    "            # 输入维度：(batch_size, 3, 128, 128)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=6, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 16, 32, 32)\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 32, 16, 16)\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 4, 4)\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 输出维度：(batch_size, 64, 2, 2)\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 2 * 2, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        x = x.view(batch_size * time_steps, channel_x, height, width)\n",
    "        x = self.conv_model(x)\n",
    "        x = x.view(batch_size * time_steps, -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch_size, time_steps, -1)\n",
    "        return x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的CNN和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用自定义的CNN和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(ConvLstm, self).__init__()\n",
    "        self.conv_model = Conv(latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size, time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义的ResNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    定义ResNet的残差块。该残差块包含两个卷积层，每个卷积层后面跟着一个批归一化层。\n",
    "    :param in_channels: 输入的通道数\n",
    "    :param out_channels: 输出的通道数\n",
    "    :param stride: 卷积的步长，默认为1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    定义ResNet模型，由多个残差块组成。\n",
    "    :param latent_dim: 输出的特征维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 8\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self.make_layer(8, 2)\n",
    "        self.layer2 = self.make_layer(16, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(32, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(64, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, latent_dim)\n",
    "\n",
    "    def make_layer(self, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size * time_steps, channel_x, height, width)\n",
    "        x = self.conv1(conv_input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用自定义的ResNet和LSTM构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResNetLstm(nn.Module):\n",
    "    \"\"\"\n",
    "    使用自定义的ResNet和LSTM构建模型\n",
    "    :param latent_dim: LSTM的输入维度\n",
    "    :param hidden_size: LSTM的隐藏层维度\n",
    "    :param lstm_layers: LSTM的层数\n",
    "    :param bidirectional: LSTM是否为双向\n",
    "    :param n_class: 分类的类别数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n",
    "        super(ResNetLstm, self).__init__()\n",
    "        self.conv_model = ResNet(latent_dim=latent_dim)\n",
    "        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size if bidirectional == True else hidden_size, n_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, channel_x, height, width = x.shape\n",
    "        conv_input = x.view(batch_size, time_steps, channel_x, height, width)\n",
    "        conv_output = self.conv_model(conv_input)\n",
    "        lstm_input = conv_output.view(batch_size, time_steps, -1)\n",
    "        lstm_output = self.Lstm(lstm_input)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        output = self.output_layer(lstm_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第三步：定义评估函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义评估函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, val_data, loss_fn, device):\n",
    "    \"\"\"\n",
    "    评估模型在验证集上的性能\n",
    "    :param model: 评估的模型\n",
    "    :param val_data: 验证数据集的数据加载器\n",
    "    :param loss_fn: 损失函数\n",
    "    :param device: 训练设备\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    # 将模型移动到设备上（如GPU）\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 在评估阶段，关闭梯度计算\n",
    "        model.eval()\n",
    "\n",
    "        # 初始化变量用于计算准确率和损失\n",
    "        val_correct = 0\n",
    "        val_total = len(val_data) * val_data.batch_size\n",
    "        running_loss = 0.\n",
    "        # 使用 tqdm 进度条显示进度\n",
    "        val_data = tqdm(val_data, desc='Evaluate: ', ncols=100)\n",
    "\n",
    "        # 遍历验证数据集\n",
    "        for data_batch, label_batch in val_data:\n",
    "            data_batch, label_batch = data_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            # 前向传播计算输出\n",
    "            output_batch = model(data_batch)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = loss_fn(output_batch, label_batch.long())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 预测标签\n",
    "            _, predicted_labels = torch.max(output_batch.data, dim=1)\n",
    "\n",
    "            # 统计正确预测的数量\n",
    "            val_correct += (label_batch == predicted_labels).sum().item()\n",
    "\n",
    "        # 计算平均损失和准确率\n",
    "        val_loss = running_loss / len(val_data)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # 返回验证集的损失和准确率\n",
    "        return val_loss, val_acc"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义可视化函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_history(history):\n",
    "    \"\"\"\n",
    "    可视化训练过程中的损失和准确率\n",
    "    :param history: 训练过程中的损失和准确率\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='train_loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='train_acc')\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第四步：定义训练函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_data, loss_fn, optimizer, epochs, device, save_last_weights_path=None,\n",
    "          save_best_weights_path=None, steps_per_epoch=None,\n",
    "          validation_data=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param model: 要训练的模型。\n",
    "    :param train_data: 训练数据集的数据加载器。\n",
    "    :param loss_fn: 损失函数。\n",
    "    :param optimizer: 优化器。\n",
    "    :param epochs: 训练的轮数。\n",
    "    :param device: 训练设备。\n",
    "    :param save_last_weights_path: 可选参数，保存最后模型权重的路径。\n",
    "    :param save_best_weights_path: 可选参数，保存最佳模型权重的路径。\n",
    "    :param steps_per_epoch: 可选参数，每个epoch的步数。\n",
    "    :param validation_data: 可选参数，用于验证的数据加载器。\n",
    "    :param scheduler: 可选参数，学习率调度器。\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    if save_best_weights_path:\n",
    "        # 评估当前模型在验证数据集上的损失\n",
    "        best_loss, _ = evaluate(model, validation_data, loss_fn, device)\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        # 如果没有指定每个epoch的步数，则将其设置为训练数据集的长度\n",
    "        steps_per_epoch = len(train_data)\n",
    "\n",
    "    num_steps = len(train_data)\n",
    "    iterator = iter(train_data)\n",
    "    count_steps = 1\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    # 将模型移动到设备上\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 遍历每个epoch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        running_loss = 0.\n",
    "        train_correct = 0\n",
    "        train_total = steps_per_epoch * train_data.batch_size\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step in tqdm(range(steps_per_epoch), desc=f'epoch: {epoch}/{epochs}: ', ncols=100):\n",
    "            img_batch, label_batch = next(iterator)\n",
    "            img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n",
    "            # 将梯度置零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播计算输出\n",
    "            output_batch = model(img_batch)\n",
    "            # 计算损失\n",
    "            loss = loss_fn(output_batch, label_batch.long())\n",
    "            # 反向传播计算梯度\n",
    "            loss.backward(retain_graph=True)\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 预测标签\n",
    "            _, predicted_labels = torch.max(output_batch.data, dim=1)\n",
    "            # 统计正确预测的数量\n",
    "            train_correct += (label_batch == predicted_labels).sum().item()\n",
    "            # 计算平均损失\n",
    "            running_loss += loss.item()\n",
    "            # 打印训练损失和准确率\n",
    "            if count_steps == num_steps:\n",
    "                # 循环迭代器，以便继续训练数据集的下一个epoch\n",
    "                count_steps = 0\n",
    "                iterator = iter(train_data)\n",
    "            count_steps += 1\n",
    "\n",
    "        train_loss = running_loss / steps_per_epoch\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        if scheduler:\n",
    "            # 如果提供了学习率调度器，则根据训练损失调整学习率\n",
    "            scheduler.step(train_loss)\n",
    "\n",
    "        history['train_loss'].append(float(train_loss))\n",
    "        history['train_acc'].append(float(train_accuracy))\n",
    "\n",
    "        # 评估模型在验证数据集上的性能\n",
    "        val_loss, val_acc = evaluate(model, validation_data, loss_fn, device)\n",
    "        # 打印训练损失和准确率\n",
    "        print(\n",
    "            f'epoch: {epoch}, train_accuracy: {train_accuracy:.2f}, loss: {train_loss:.3f}, val_accuracy: {val_acc:.2f}, val_loss: {val_loss:.3f}')\n",
    "\n",
    "        if save_best_weights_path:\n",
    "            if val_loss < best_loss:\n",
    "                # 如果验证损失更小，则保存模型的权重\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_best_weights_path)\n",
    "                print(f'Saved successfully best weights to:', save_best_weights_path)\n",
    "        history['val_loss'].append(float(val_loss))\n",
    "        history['val_acc'].append(float(val_acc))\n",
    "\n",
    "    if save_last_weights_path:\n",
    "        # 如果提供了保存最后权重的路径，则保存模型的权重\n",
    "        torch.save(model.state_dict(), save_last_weights_path)\n",
    "        print(f'Saved successfully last weights to:', save_last_weights_path)\n",
    "\n",
    "    return model, history"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第五步：训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 指定模型参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 固定的参数\n",
    "num_classes = 10\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 可调整的参数\n",
    "num_frames = 15  # You can adjust this to balance speed and accuracy\n",
    "img_size = (128, 128)  # You can adjust this to balance speed and accuracy\n",
    "latent_dim = 2048\n",
    "hid_size = 128\n",
    "num_lstm_layers = 2\n",
    "learning_rate = 2e-5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数据预处理的转换流程。\n",
    "# 使用albumentations库进行图像处理，包括图像大小调整、归一化和转换为张量。\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(height=img_size[0], width=img_size[1]),\n",
    "        albumentations.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info('Loading dataset')\n",
    "# 加载数据集并指定数据集的路径、帧数、类别数和数据预处理的转换函数。\n",
    "full_dataset = VideoDataset(data_dir=\"/kaggle/input/ucf-101-dataset-extract-10/data\", num_class=num_classes,\n",
    "                            num_frame=num_frames, transform=transform)\n",
    "# 将数据集分为训练集和测试集，其中测试集的比例为0.2。\n",
    "train_dataset, test_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "# 使用PyTorch的DataLoader加载数据集。\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "logger.info('Dataset loaded')\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PretrainedConvLstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPretrainedConvLstm\u001B[49m(latent_dim\u001B[38;5;241m=\u001B[39mlatent_dim, hidden_size\u001B[38;5;241m=\u001B[39mhid_size, lstm_layers\u001B[38;5;241m=\u001B[39mnum_lstm_layers, bidirectional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      2\u001B[0m                            n_class\u001B[38;5;241m=\u001B[39mnum_classes)\n\u001B[0;32m      4\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlearning_rate)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'PretrainedConvLstm' is not defined"
     ]
    }
   ],
   "source": [
    "model = PretrainedConvLstm(latent_dim=latent_dim, hidden_size=hid_size, lstm_layers=num_lstm_layers, bidirectional=True,\n",
    "                           n_class=num_classes)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, mode='min', patience=3, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T02:49:47.255299200Z",
     "start_time": "2023-12-20T02:49:47.099569900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, history = train(model, train_loader, loss_fn, optimizer, epochs=70, device=device,\n",
    "                       save_last_weights_path='/kaggle/working/last_model.pth',\n",
    "                       save_best_weights_path='/kaggle/working/last_model.pth', validation_data=test_loader,\n",
    "                       scheduler=scheduler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第六步：评估模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_history(history)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, val_data=test_loader, loss_fn=loss_fn, device=device)\n",
    "print(f'Loss: {test_loss : .3f}, Acc: {test_acc: .3f}')\n",
    "\n",
    "test_loss, test_acc = evaluate(model, val_data=train_loader, loss_fn=loss_fn, device=device)\n",
    "print(f'Loss: {test_loss : .3f}, Acc: {test_acc: .3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
